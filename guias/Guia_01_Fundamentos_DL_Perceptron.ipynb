{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guia 01: Fundamentos de Deep Learning - El Perceptron\n",
    "\n",
    "## Electiva II - Deep Learning | Tecnologico de Antioquia\n",
    "\n",
    "---\n",
    "\n",
    "**Programa:** Ingenieria en Software \n",
    "**Asignatura:** Electiva II - Deep Learning \n",
    "**Guia:** 01 de 14 \n",
    "**Autor:** Profesor Julian Florez \n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo de Aprendizaje\n",
    "\n",
    "Al finalizar esta guia, el estudiante estara en capacidad de:\n",
    "\n",
    "- Comprender que es Deep Learning y su relacion con Machine Learning.\n",
    "- Entender el funcionamiento del **Perceptron**, la unidad fundamental de las redes neuronales.\n",
    "- Implementar un Perceptron desde cero utilizando NumPy.\n",
    "- Experimentar con diferentes tasas de aprendizaje y funciones de activacion.\n",
    "- Identificar las **limitaciones** del Perceptron simple (problema XOR).\n",
    "- Comprender por que se necesitan redes multicapa.\n",
    "\n",
    "### Concepto Nuevo Introducido\n",
    "\n",
    "**El Perceptron:** La neurona artificial mas simple, propuesta por Frank Rosenblatt en 1958. Es el bloque fundamental sobre el cual se construyen todas las redes neuronales profundas modernas.\n",
    "\n",
    "### Duracion Estimada\n",
    "\n",
    "**3 horas** (incluye lectura teorica, experimentacion y preguntas de reflexion)\n",
    "\n",
    "---\n",
    "\n",
    "> **IMPORTANTE:** Las secciones marcadas con **\\u270d\\ufe0f** requieren tu respuesta escrita. Estas respuestas son parte de tu evaluacion. No las dejes en blanco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuracion del Entorno\n",
    "\n",
    "Antes de comenzar, necesitamos importar las librerias que usaremos a lo largo de toda la guia. Ejecuta la siguiente celda para asegurarte de que todo este correctamente instalado."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CONFIGURACION DEL ENTORNO\n",
    "# ============================================================\n",
    "\n",
    "# Libreria fundamental para operaciones numericas y manejo de arrays\n",
    "import numpy as np\n",
    "\n",
    "# Libreria para visualizacion de datos y graficos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Modulos de scikit-learn para generacion de datos sinteticos y metricas\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Configurar warnings para que no ensucien la salida\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# SEMILLA DE REPRODUCIBILIDAD\n",
    "# ============================================================\n",
    "# Fijamos la semilla para que todos obtengamos los mismos resultados.\n",
    "# Esto es fundamental en investigacion y en entornos educativos.\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACION GLOBAL DE GRAFICOS\n",
    "# ============================================================\n",
    "# Hacemos que los graficos se vean mas grandes y legibles\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Entorno configurado correctamente.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Todas las librerias fueron importadas sin errores.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Marco Teorico\n",
    "\n",
    "### 2.1 \\u00bfQue es Deep Learning?\n",
    "\n",
    "En el curso anterior de Machine Learning, aprendiste a construir modelos que aprenden patrones a partir de datos. Usaste algoritmos como regresion logistica, arboles de decision, SVM y k-NN. Estos algoritmos funcionan muy bien para muchos problemas, pero tienen una limitacion importante: **requieren que el humano seleccione y disene las caracteristicas (features)** que el modelo va a utilizar.\n",
    "\n",
    "**Deep Learning** (Aprendizaje Profundo) es una rama del Machine Learning que utiliza **redes neuronales artificiales con multiples capas** para aprender automaticamente representaciones jerarquicas de los datos. En otras palabras:\n",
    "\n",
    "- En **Machine Learning clasico**, tu decides que features usar (por ejemplo, el largo del petalo, el ancho del sepalo).\n",
    "- En **Deep Learning**, la red neuronal **aprende sola** cuales son las mejores features, directamente a partir de los datos crudos.\n",
    "\n",
    "#### Jerarquia de conceptos:\n",
    "\n",
    "```\n",
    "Inteligencia Artificial (IA)\n",
    "    |__ Machine Learning (ML)\n",
    "            |__ Deep Learning (DL)\n",
    "                    |__ Redes Neuronales Convolucionales (CNN)\n",
    "                    |__ Redes Neuronales Recurrentes (RNN)\n",
    "                    |__ Transformers\n",
    "                    |__ ... y muchas mas arquitecturas\n",
    "```\n",
    "\n",
    "Deep Learning ha revolucionado campos como:\n",
    "- **Vision por computador:** reconocimiento facial, diagnostico medico por imagenes, conduccion autonoma.\n",
    "- **Procesamiento de lenguaje natural:** traductores automaticos, chatbots, modelos de lenguaje (como GPT o Claude).\n",
    "- **Audio:** reconocimiento de voz, generacion de musica, asistentes virtuales.\n",
    "- **Juegos:** AlphaGo derrotando campeones mundiales de Go.\n",
    "\n",
    "Pero todo comienza con una idea muy simple: **la neurona artificial**, tambien llamada **Perceptron**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 La Neurona Biologica vs la Neurona Artificial\n",
    "\n",
    "Las redes neuronales artificiales estan **inspiradas** (no son copias exactas) en el funcionamiento del cerebro humano. Veamos la analogia:\n",
    "\n",
    "#### La neurona biologica:\n",
    "\n",
    "Tu cerebro contiene aproximadamente **86 mil millones** de neuronas, cada una conectada con miles de otras neuronas. Una neurona biologica funciona asi:\n",
    "\n",
    "1. **Dendritas:** Reciben senales electricas de otras neuronas.\n",
    "2. **Soma (cuerpo celular):** Procesa y acumula las senales recibidas.\n",
    "3. **Axon:** Si la acumulacion de senales supera un **umbral**, la neurona se \"dispara\" y envia una senal a traves del axon.\n",
    "4. **Sinapsis:** La conexion entre el axon de una neurona y la dendrita de otra. Cada sinapsis tiene una \"fuerza\" diferente.\n",
    "\n",
    "#### La neurona artificial (Perceptron):\n",
    "\n",
    "| Componente Biologico | Componente Artificial | Descripcion |\n",
    "|---|---|---|\n",
    "| Dendritas | Entradas ($x_1, x_2, ..., x_n$) | Los datos que recibe la neurona |\n",
    "| Fuerza sinaptica | Pesos ($w_1, w_2, ..., w_n$) | La importancia de cada entrada |\n",
    "| Soma | Suma ponderada ($\\sum w_i x_i + b$) | Acumula las entradas multiplicadas por sus pesos |\n",
    "| Umbral de disparo | Funcion de activacion ($f$) | Decide si la neurona se \"activa\" o no |\n",
    "| Axon | Salida ($\\hat{y}$) | El resultado de la neurona |\n",
    "\n",
    "La diferencia clave es que una neurona biologica es enormemente compleja, mientras que la neurona artificial es una simplificacion matematica que captura la esencia del proceso: **recibir entradas, ponderarlas, sumarlas y decidir si activarse o no**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 El Perceptron de Rosenblatt\n",
    "\n",
    "El **Perceptron** fue propuesto por **Frank Rosenblatt** en 1958 en el Cornell Aeronautical Laboratory. Es el modelo mas simple de neurona artificial y fue disenado originalmente para **clasificacion binaria** (separar datos en dos clases).\n",
    "\n",
    "#### \\u00bfComo funciona el Perceptron?\n",
    "\n",
    "El Perceptron realiza los siguientes pasos:\n",
    "\n",
    "**Paso 1: Recibir las entradas**\n",
    "\n",
    "El Perceptron recibe un vector de entradas $\\mathbf{x} = [x_1, x_2, ..., x_n]$ donde cada $x_i$ es una caracteristica del dato.\n",
    "\n",
    "**Paso 2: Calcular la suma ponderada**\n",
    "\n",
    "Cada entrada se multiplica por un peso correspondiente y se suma un termino de sesgo (bias):\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i \\cdot x_i + b = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$$\n",
    "\n",
    "En notacion vectorial:\n",
    "\n",
    "$$z = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_n]$ es el vector de pesos\n",
    "- $\\mathbf{x} = [x_1, x_2, ..., x_n]$ es el vector de entradas\n",
    "- $b$ es el bias (sesgo)\n",
    "\n",
    "**Paso 3: Aplicar la funcion de activacion**\n",
    "\n",
    "La suma ponderada $z$ se pasa a traves de una funcion de activacion $f(z)$ para obtener la salida:\n",
    "\n",
    "$$\\hat{y} = f(z)$$\n",
    "\n",
    "En el Perceptron original, la funcion de activacion es la **funcion escalon** (step function):\n",
    "\n",
    "$$f(z) = \\begin{cases} 1 & \\text{si } z \\geq 0 \\\\ 0 & \\text{si } z < 0 \\end{cases}$$\n",
    "\n",
    "**Paso 4: Comparar con la etiqueta real y actualizar pesos**\n",
    "\n",
    "Si la prediccion $\\hat{y}$ no coincide con la etiqueta real $y$, los pesos se actualizan usando la **regla de aprendizaje del Perceptron**:\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\cdot (y - \\hat{y}) \\cdot x_i$$\n",
    "\n",
    "$$b \\leftarrow b + \\eta \\cdot (y - \\hat{y})$$\n",
    "\n",
    "Donde $\\eta$ (eta) es la **tasa de aprendizaje** (learning rate), un hiperparametro que controla que tan grandes son los ajustes en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Funciones de Activacion\n",
    "\n",
    "Las funciones de activacion son un componente fundamental de las redes neuronales. Introducen **no linealidad** en el modelo, lo que le permite aprender patrones complejos. Sin ellas, una red neuronal de cualquier profundidad seria equivalente a una sola transformacion lineal.\n",
    "\n",
    "Veamos las mas importantes:\n",
    "\n",
    "#### 1. Funcion Escalon (Step Function)\n",
    "\n",
    "Es la funcion original del Perceptron. Es binaria: la salida es 0 o 1.\n",
    "\n",
    "$$f(z) = \\begin{cases} 1 & \\text{si } z \\geq 0 \\\\ 0 & \\text{si } z < 0 \\end{cases}$$\n",
    "\n",
    "- **Ventaja:** Simple e intuitiva.\n",
    "- **Desventaja:** No es diferenciable en $z = 0$. No proporciona gradiente util para el aprendizaje basado en gradiente (backpropagation).\n",
    "\n",
    "#### 2. Funcion Sigmoide (Logistica)\n",
    "\n",
    "Transforma cualquier valor real a un numero entre 0 y 1. Es suave y diferenciable.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Su derivada es:\n",
    "\n",
    "$$\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$$\n",
    "\n",
    "- **Ventaja:** Diferenciable, salida interpretable como probabilidad.\n",
    "- **Desventaja:** Problema de **gradiente desvaneciente** (vanishing gradient) en valores muy positivos o muy negativos.\n",
    "\n",
    "#### 3. Funcion Tangente Hiperbolica (tanh)\n",
    "\n",
    "Similar a la sigmoide pero con rango entre -1 y 1.\n",
    "\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "- **Ventaja:** Centrada en cero, lo que puede acelerar el entrenamiento.\n",
    "- **Desventaja:** Tambien sufre de gradiente desvaneciente.\n",
    "\n",
    "#### 4. Funcion ReLU (Rectified Linear Unit)\n",
    "\n",
    "La funcion de activacion mas popular en Deep Learning moderno.\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max(0, z) = \\begin{cases} z & \\text{si } z > 0 \\\\ 0 & \\text{si } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "- **Ventaja:** Computacionalmente eficiente, no sufre de gradiente desvaneciente (para valores positivos), acelera la convergencia.\n",
    "- **Desventaja:** Puede producir \"neuronas muertas\" (cuando la entrada siempre es negativa, la neurona deja de aprender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 El Proceso de Aprendizaje\n",
    "\n",
    "\\u00bfComo \"aprende\" un Perceptron? El proceso es iterativo y se puede resumir en estos pasos:\n",
    "\n",
    "1. **Inicializacion:** Los pesos $\\mathbf{w}$ y el bias $b$ se inicializan con valores pequenos aleatorios (o en ceros).\n",
    "\n",
    "2. **Forward pass (pasada hacia adelante):** Se calcula la prediccion para cada ejemplo de entrenamiento.\n",
    "\n",
    "3. **Calculo del error:** Se compara la prediccion $\\hat{y}$ con la etiqueta real $y$.\n",
    "\n",
    "4. **Actualizacion de pesos:** Si hay error, se ajustan los pesos en la direccion que reduce el error.\n",
    "\n",
    "5. **Repetir:** Se repiten los pasos 2-4 por multiples **epocas** (pasadas completas por todos los datos).\n",
    "\n",
    "#### Conceptos clave:\n",
    "\n",
    "- **Peso ($w_i$):** Representa la importancia o influencia de la entrada $x_i$. Un peso grande (positivo o negativo) significa que esa entrada tiene mucha influencia en la decision. Un peso cercano a cero significa que esa entrada es poco relevante.\n",
    "\n",
    "- **Bias ($b$):** Es un termino de desplazamiento que permite mover la frontera de decision. Sin el bias, la frontera de decision siempre pasaria por el origen. Es analogo al intercepto en una regresion lineal.\n",
    "\n",
    "- **Tasa de aprendizaje ($\\eta$):** Controla el tamano de los pasos que damos al actualizar los pesos.\n",
    "  - **Muy alta:** El modelo puede \"saltar\" por encima del optimo y no converger (oscilaciones).\n",
    "  - **Muy baja:** El modelo converge muy lentamente, puede requerir muchas epocas.\n",
    "  - **Adecuada:** El modelo converge de manera estable hacia una buena solucion.\n",
    "\n",
    "- **Epoca:** Una pasada completa por todo el conjunto de datos de entrenamiento. Tipicamente, el modelo necesita varias epocas para aprender.\n",
    "\n",
    "- **Convergencia:** Se dice que el modelo ha convergido cuando el error deja de disminuir significativamente o cuando clasifica correctamente todos los ejemplos de entrenamiento.\n",
    "\n",
    "#### Teorema de convergencia del Perceptron:\n",
    "\n",
    "Un resultado teorico importante es que **si los datos son linealmente separables, el Perceptron siempre convergera** a una solucion en un numero finito de pasos. Sin embargo, si los datos **no** son linealmente separables, el Perceptron nunca convergera; seguira oscilando indefinidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Carga y Exploracion de Datos\n",
    "\n",
    "Para nuestros experimentos, vamos a generar un dataset sintetico de clasificacion binaria. Usaremos `sklearn.datasets.make_blobs` para crear dos grupos (clusters) de puntos en 2D que sean **linealmente separables** (es decir, que se puedan separar con una linea recta).\n",
    "\n",
    "\\u00bfPor que datos sinteticos? Porque nos permiten:\n",
    "- Controlar exactamente como lucen los datos.\n",
    "- Saber de antemano si el problema es resoluble.\n",
    "- Enfocarnos en entender el algoritmo sin preocuparnos por el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GENERACION DEL DATASET SINTETICO\n",
    "# ============================================================\n",
    "\n",
    "# Generamos 200 puntos en 2D, divididos en 2 clases (clusters)\n",
    "# - n_samples: numero total de muestras\n",
    "# - centers: numero de clusters (clases)\n",
    "# - n_features: numero de caracteristicas (2 para poder visualizar en 2D)\n",
    "# - cluster_std: desviacion estandar de cada cluster (controla la dispersion)\n",
    "# - random_state: semilla para reproducibilidad\n",
    "X, y = make_blobs(\n",
    "    n_samples=200,\n",
    "    centers=2,\n",
    "    n_features=2,\n",
    "    cluster_std=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Veamos la forma de nuestros datos\n",
    "print(\"Forma de X (datos):\", X.shape)\n",
    "print(\"Forma de y (etiquetas):\", y.shape)\n",
    "print(\"\\nPrimeros 5 ejemplos:\")\n",
    "print(\"X:\")\n",
    "print(X[:5])\n",
    "print(\"\\ny (etiquetas):\")\n",
    "print(y[:5])\n",
    "print(f\"\\nClases unicas: {np.unique(y)}\")\n",
    "print(f\"Cantidad por clase: Clase 0 = {np.sum(y == 0)}, Clase 1 = {np.sum(y == 1)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION DEL DATASET\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "\n",
    "# Scatter plot coloreado por clase\n",
    "# Clase 0 en azul, Clase 1 en rojo\n",
    "scatter = ax.scatter(\n",
    "    X[:, 0],          # Feature 1 (eje X)\n",
    "    X[:, 1],          # Feature 2 (eje Y)\n",
    "    c=y,              # Color segun la clase\n",
    "    cmap='bwr',       # Mapa de colores: azul-blanco-rojo\n",
    "    edgecolors='k',   # Borde negro en cada punto\n",
    "    s=80,             # Tamano de los puntos\n",
    "    alpha=0.8         # Transparencia\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Feature 1 ($x_1$)', fontsize=14)\n",
    "ax.set_ylabel('Feature 2 ($x_2$)', fontsize=14)\n",
    "ax.set_title('Dataset Sintetico de Clasificacion Binaria', fontsize=16)\n",
    "ax.legend(*scatter.legend_elements(), title='Clase', fontsize=12, title_fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserva que los dos grupos de puntos estan claramente separados.\")\n",
    "print(\"Un Perceptron deberia poder encontrar una linea que los separe.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Experimentacion Guiada\n",
    "\n",
    "Ahora viene la parte mas importante de esta guia. Vamos a realizar cinco experimentos para entender profundamente como funciona el Perceptron, cuales son sus fortalezas y cuales son sus limitaciones.\n",
    "\n",
    "---\n",
    "\n",
    "### EXPERIMENTO 1: Implementar un Perceptron desde Cero\n",
    "\n",
    "Vamos a implementar un Perceptron completo utilizando **solo NumPy**. Esto es fundamental para entender que hay \"debajo del capo\" de las librerias como TensorFlow o PyTorch que usaremos mas adelante en el curso.\n",
    "\n",
    "La clase `Perceptron` tendra dos metodos principales:\n",
    "- `fit(X, y)`: Entrena el modelo ajustando los pesos.\n",
    "- `predict(X)`: Realiza predicciones con los pesos aprendidos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 1: IMPLEMENTACION DEL PERCEPTRON DESDE CERO\n",
    "# ============================================================\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Implementacion de un Perceptron simple desde cero.\n",
    "    \n",
    "    Parametros:\n",
    "    -----------\n",
    "    learning_rate : float\n",
    "        Tasa de aprendizaje (eta). Controla el tamano de los ajustes\n",
    "        en cada actualizacion de pesos. Por defecto es 0.01.\n",
    "    n_epochs : int\n",
    "        Numero maximo de epocas (pasadas por los datos). Por defecto 100.\n",
    "    \n",
    "    Atributos despues de entrenar:\n",
    "    ------------------------------\n",
    "    weights_ : array de pesos aprendidos\n",
    "    bias_ : float, bias aprendido\n",
    "    errors_ : lista con el numero de errores por epoca\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100):\n",
    "        # Guardamos los hiperparametros\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        Funcion de activacion escalon (step function).\n",
    "        Retorna 1 si z >= 0, de lo contrario retorna 0.\n",
    "        \"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Entrena el Perceptron con los datos X y las etiquetas y.\n",
    "        \n",
    "        Parametros:\n",
    "        -----------\n",
    "        X : array de forma (n_muestras, n_features)\n",
    "            Datos de entrenamiento.\n",
    "        y : array de forma (n_muestras,)\n",
    "            Etiquetas de clase (0 o 1).\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        self : para permitir encadenamiento de metodos\n",
    "        \"\"\"\n",
    "        n_muestras, n_features = X.shape\n",
    "        \n",
    "        # Paso 1: Inicializar pesos en ceros\n",
    "        # Un peso por cada feature\n",
    "        self.weights_ = np.zeros(n_features)\n",
    "        \n",
    "        # Inicializar bias en cero\n",
    "        self.bias_ = 0.0\n",
    "        \n",
    "        # Lista para guardar el numero de errores en cada epoca\n",
    "        self.errors_ = []\n",
    "        \n",
    "        # Paso 2: Iterar por cada epoca\n",
    "        for epoca in range(self.n_epochs):\n",
    "            errores_epoca = 0\n",
    "            \n",
    "            # Paso 3: Iterar por cada ejemplo de entrenamiento\n",
    "            for xi, yi in zip(X, y):\n",
    "                # Paso 3a: Calcular la suma ponderada (z = w^T * x + b)\n",
    "                z = np.dot(xi, self.weights_) + self.bias_\n",
    "                \n",
    "                # Paso 3b: Aplicar funcion de activacion\n",
    "                y_pred = self.activation(z)\n",
    "                \n",
    "                # Paso 3c: Calcular el error (diferencia entre real y predicho)\n",
    "                error = yi - y_pred\n",
    "                \n",
    "                # Paso 3d: Actualizar pesos y bias si hay error\n",
    "                # w_i = w_i + eta * (y - y_pred) * x_i\n",
    "                self.weights_ += self.learning_rate * error * xi\n",
    "                \n",
    "                # b = b + eta * (y - y_pred)\n",
    "                self.bias_ += self.learning_rate * error\n",
    "                \n",
    "                # Contar errores (si error != 0, hubo clasificacion incorrecta)\n",
    "                errores_epoca += int(error != 0)\n",
    "            \n",
    "            # Guardar el numero de errores de esta epoca\n",
    "            self.errors_.append(errores_epoca)\n",
    "            \n",
    "            # Si no hubo errores, el modelo ya convergio\n",
    "            if errores_epoca == 0:\n",
    "                print(f\"Convergencia alcanzada en la epoca {epoca + 1}\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Realiza predicciones para un conjunto de datos X.\n",
    "        \n",
    "        Parametros:\n",
    "        -----------\n",
    "        X : array de forma (n_muestras, n_features)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        array de predicciones (0 o 1)\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.weights_) + self.bias_\n",
    "        return self.activation(z)\n",
    "\n",
    "\n",
    "print(\"Clase Perceptron definida correctamente.\")\n",
    "print(\"Ahora vamos a entrenarla con nuestro dataset sintetico.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# ENTRENAMIENTO DEL PERCEPTRON\n",
    "# ============================================================\n",
    "\n",
    "# Crear una instancia del Perceptron con tasa de aprendizaje 0.01 y 100 epocas\n",
    "perceptron = Perceptron(learning_rate=0.01, n_epochs=100)\n",
    "\n",
    "# Entrenar el modelo con nuestro dataset\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Mostrar los pesos y bias aprendidos\n",
    "print(f\"\\nPesos aprendidos: w1 = {perceptron.weights_[0]:.4f}, w2 = {perceptron.weights_[1]:.4f}\")\n",
    "print(f\"Bias aprendido: b = {perceptron.bias_:.4f}\")\n",
    "\n",
    "# Calcular la precision (accuracy) del modelo\n",
    "predicciones = perceptron.predict(X)\n",
    "accuracy = accuracy_score(y, predicciones)\n",
    "print(f\"\\nAccuracy en entrenamiento: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Mostrar la cantidad de errores por epoca\n",
    "print(f\"\\nErrores por epoca: {perceptron.errors_}\")\n",
    "print(f\"Total de epocas usadas: {len(perceptron.errors_)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION: ERRORES POR EPOCA\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(\n",
    "    range(1, len(perceptron.errors_) + 1),  # Eje X: numero de epoca\n",
    "    perceptron.errors_,                      # Eje Y: errores en esa epoca\n",
    "    marker='o',                              # Marcador circular\n",
    "    linestyle='-',                           # Linea continua\n",
    "    color='steelblue',                       # Color de la linea\n",
    "    linewidth=2,\n",
    "    markersize=8\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Epoca', fontsize=14)\n",
    "ax.set_ylabel('Numero de clasificaciones incorrectas', fontsize=14)\n",
    "ax.set_title('Convergencia del Perceptron', fontsize=16)\n",
    "ax.set_xticks(range(1, len(perceptron.errors_) + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION: FRONTERA DE DECISION\n",
    "# ============================================================\n",
    "\n",
    "def plot_decision_boundary(X, y, model, title=\"Frontera de Decision\", ax=None):\n",
    "    \"\"\"\n",
    "    Funcion auxiliar para visualizar la frontera de decision de un modelo.\n",
    "    \n",
    "    Parametros:\n",
    "    -----------\n",
    "    X : array (n_muestras, 2) - datos con 2 features\n",
    "    y : array (n_muestras,) - etiquetas\n",
    "    model : modelo con metodo predict()\n",
    "    title : titulo del grafico\n",
    "    ax : eje de matplotlib (opcional)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Crear una malla de puntos que cubra todo el espacio de features\n",
    "    # con un pequeno margen extra\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Crear una cuadricula de puntos separados por 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, 0.02),\n",
    "        np.arange(y_min, y_max, 0.02)\n",
    "    )\n",
    "    \n",
    "    # Predecir la clase para cada punto de la cuadricula\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Pintar las regiones de decision\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=1)\n",
    "    \n",
    "    # Dibujar los puntos de datos\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', s=60, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1 ($x_1$)', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2 ($x_2$)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "\n",
    "# Visualizar la frontera de decision del Perceptron entrenado\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plot_decision_boundary(X, y, perceptron, \n",
    "                       title='Frontera de Decision del Perceptron', ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"La linea negra es la frontera de decision.\")\n",
    "print(\"Los puntos de un lado son clasificados como clase 0, los del otro como clase 1.\")\n",
    "print(\"\\nObserva que la frontera es una LINEA RECTA.\")\n",
    "print(\"El Perceptron solo puede aprender fronteras lineales.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta del Experimento 1\n",
    "\n",
    "**\\u00bfCuantas epocas necesito el modelo para converger? \\u00bfPor que crees que fue ese numero?**\n",
    "\n",
    "Pistas para tu reflexion:\n",
    "- Observa la grafica de errores por epoca.\n",
    "- Piensa en la separabilidad de los datos.\n",
    "- \\u00bfQue pasaria si los clusters estuvieran mas juntos (mayor overlap)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 2: Efecto de la Tasa de Aprendizaje\n",
    "\n",
    "La **tasa de aprendizaje** ($\\eta$) es uno de los hiperparametros mas importantes en cualquier modelo de aprendizaje automatico. Es el factor que controla **que tan grandes son los pasos** que da el modelo al actualizar sus pesos.\n",
    "\n",
    "Imaginalo asi:\n",
    "- Estas bajando una montana con los ojos vendados.\n",
    "- La tasa de aprendizaje es **el tamano de cada paso** que das.\n",
    "- Si das pasos muy grandes, puedes pasar de largo el valle (el minimo) y terminar en otra montana.\n",
    "- Si das pasos muy pequenos, vas a tardar una eternidad en llegar al fondo.\n",
    "- Necesitas un tamano de paso que sea \"justo\" para llegar al fondo de manera eficiente.\n",
    "\n",
    "Vamos a entrenar el Perceptron con 4 tasas de aprendizaje diferentes y comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 2: EFECTO DE LA TASA DE APRENDIZAJE\n",
    "# ============================================================\n",
    "\n",
    "# Definimos las tasas de aprendizaje que vamos a probar\n",
    "tasas_aprendizaje = [0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "# Colores para cada tasa\n",
    "colores = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "# Almacenar los modelos entrenados\n",
    "modelos = {}\n",
    "\n",
    "# ---- Grafico 1: Errores por epoca para cada tasa ----\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for eta, color in zip(tasas_aprendizaje, colores):\n",
    "    # Crear y entrenar un Perceptron con la tasa de aprendizaje actual\n",
    "    modelo = Perceptron(learning_rate=eta, n_epochs=50)\n",
    "    modelo.fit(X, y)\n",
    "    modelos[eta] = modelo\n",
    "    \n",
    "    # Graficar los errores por epoca\n",
    "    epocas = range(1, len(modelo.errors_) + 1)\n",
    "    ax.plot(epocas, modelo.errors_, marker='o', linestyle='-', color=color,\n",
    "            linewidth=2, markersize=6, label=f'$\\\\eta$ = {eta}')\n",
    "\n",
    "ax.set_xlabel('Epoca', fontsize=14)\n",
    "ax.set_ylabel('Numero de errores', fontsize=14)\n",
    "ax.set_title('Efecto de la Tasa de Aprendizaje en la Convergencia', fontsize=16)\n",
    "ax.legend(fontsize=12, title='Tasa de Aprendizaje', title_fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resumen\n",
    "print(\"\\nResumen de convergencia:\")\n",
    "print(\"-\" * 50)\n",
    "for eta in tasas_aprendizaje:\n",
    "    modelo = modelos[eta]\n",
    "    epocas_usadas = len(modelo.errors_)\n",
    "    error_final = modelo.errors_[-1]\n",
    "    acc = accuracy_score(y, modelo.predict(X))\n",
    "    print(f\"eta = {eta:>5} | Epocas: {epocas_usadas:>3} | Error final: {error_final:>3} | Accuracy: {acc*100:.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION: FRONTERAS DE DECISION PARA CADA TASA\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "for idx, (eta, ax) in enumerate(zip(tasas_aprendizaje, axes.flatten())):\n",
    "    modelo = modelos[eta]\n",
    "    acc = accuracy_score(y, modelo.predict(X))\n",
    "    plot_decision_boundary(\n",
    "        X, y, modelo,\n",
    "        title=f'$\\\\eta$ = {eta} | Accuracy = {acc*100:.1f}% | Epocas = {len(modelo.errors_)}',\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "plt.suptitle('Fronteras de Decision para Diferentes Tasas de Aprendizaje', \n",
    "             fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta del Experimento 2\n",
    "\n",
    "**\\u00bfQue sucede con tasas de aprendizaje muy altas? \\u00bfY con tasas muy bajas? \\u00bfCual consideras la mejor y por que?**\n",
    "\n",
    "Pistas para tu reflexion:\n",
    "- Observa la grafica de errores por epoca. \\u00bfCual converge mas rapido?\n",
    "- \\u00bfAlguna tasa no logra converger en 50 epocas?\n",
    "- \\u00bfLa frontera de decision es diferente para cada tasa?\n",
    "- Piensa en el compromiso entre velocidad de convergencia y estabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 3: Funciones de Activacion\n",
    "\n",
    "Las funciones de activacion determinan **como se transforma la suma ponderada** en la salida de la neurona. En este experimento, primero vamos a visualizar las cuatro funciones de activacion principales y luego vamos a modificar nuestro Perceptron para usar la **sigmoide** en lugar de la funcion escalon.\n",
    "\n",
    "#### \\u00bfPor que son importantes?\n",
    "\n",
    "Sin una funcion de activacion no lineal, una red neuronal (sin importar cuantas capas tenga) solo podria aprender transformaciones lineales. Las funciones de activacion permiten que el modelo capture **patrones no lineales** en los datos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 3 - PARTE 1: VISUALIZACION DE FUNCIONES DE ACTIVACION\n",
    "# ============================================================\n",
    "\n",
    "# Definimos cada funcion de activacion\n",
    "def step_function(z):\n",
    "    \"\"\"Funcion escalon: 1 si z >= 0, 0 en caso contrario\"\"\"\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Funcion sigmoide: 1 / (1 + e^(-z))\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh_func(z):\n",
    "    \"\"\"Funcion tangente hiperbolica: (e^z - e^(-z)) / (e^z + e^(-z))\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"Funcion ReLU: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Valores de entrada para graficar\n",
    "z = np.linspace(-6, 6, 300)\n",
    "\n",
    "# Crear los 4 subgraficos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ---- Funcion Escalon ----\n",
    "axes[0, 0].plot(z, step_function(z), color='#e74c3c', linewidth=3)\n",
    "axes[0, 0].set_title('Funcion Escalon (Step)', fontsize=14)\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel('f(z)')\n",
    "axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].set_ylim(-0.1, 1.1)\n",
    "axes[0, 0].text(2, 0.5, r'$f(z) = \\begin{cases} 1 & z \\geq 0 \\\\ 0 & z < 0 \\end{cases}$',\n",
    "                fontsize=13, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# ---- Funcion Sigmoide ----\n",
    "axes[0, 1].plot(z, sigmoid(z), color='#3498db', linewidth=3)\n",
    "axes[0, 1].set_title('Funcion Sigmoide', fontsize=14)\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('$\\\\sigma(z)$')\n",
    "axes[0, 1].axhline(y=0.5, color='gray', linewidth=0.5, linestyle='--')\n",
    "axes[0, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0, 1].set_ylim(-0.1, 1.1)\n",
    "axes[0, 1].text(1.5, 0.3, r'$\\sigma(z) = \\frac{1}{1 + e^{-z}}$',\n",
    "                fontsize=14, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# ---- Funcion Tanh ----\n",
    "axes[1, 0].plot(z, tanh_func(z), color='#2ecc71', linewidth=3)\n",
    "axes[1, 0].set_title('Funcion Tangente Hiperbolica (tanh)', fontsize=14)\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel('tanh(z)')\n",
    "axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].set_ylim(-1.2, 1.2)\n",
    "axes[1, 0].text(1.5, -0.5, r'$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$',\n",
    "                fontsize=13, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# ---- Funcion ReLU ----\n",
    "axes[1, 1].plot(z, relu(z), color='#f39c12', linewidth=3)\n",
    "axes[1, 1].set_title('Funcion ReLU', fontsize=14)\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel('ReLU(z)')\n",
    "axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].text(1.5, 3, r'$\\text{ReLU}(z) = \\max(0, z)$',\n",
    "                fontsize=14, bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Funciones de Activacion en Redes Neuronales', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observa las diferencias entre cada funcion:\")\n",
    "print(\"- Escalon: salida binaria (0 o 1), cambio abrupto.\")\n",
    "print(\"- Sigmoide: salida continua entre 0 y 1, transicion suave.\")\n",
    "print(\"- Tanh: salida continua entre -1 y 1, centrada en cero.\")\n",
    "print(\"- ReLU: salida 0 para negativos, lineal para positivos.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 3 - PARTE 2: PERCEPTRON CON ACTIVACION SIGMOIDE\n",
    "# ============================================================\n",
    "\n",
    "class PerceptronSigmoide:\n",
    "    \"\"\"\n",
    "    Perceptron con funcion de activacion sigmoide.\n",
    "    A diferencia del Perceptron clasico con funcion escalon,\n",
    "    este modelo produce salidas continuas entre 0 y 1.\n",
    "    \n",
    "    La regla de actualizacion se mantiene igual, pero la prediccion\n",
    "    ahora pasa por la sigmoide y se umbraliza a 0.5 para clasificacion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Funcion sigmoide: mapea cualquier valor real a (0, 1)\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el perceptron con activacion sigmoide.\"\"\"\n",
    "        n_muestras, n_features = X.shape\n",
    "        \n",
    "        # Inicializar pesos y bias\n",
    "        self.weights_ = np.zeros(n_features)\n",
    "        self.bias_ = 0.0\n",
    "        self.errors_ = []\n",
    "        self.losses_ = []  # Tambien vamos a guardar el loss (error cuadratico medio)\n",
    "        \n",
    "        for epoca in range(self.n_epochs):\n",
    "            errores_epoca = 0\n",
    "            loss_epoca = 0\n",
    "            \n",
    "            for xi, yi in zip(X, y):\n",
    "                # Calcular la suma ponderada\n",
    "                z = np.dot(xi, self.weights_) + self.bias_\n",
    "                \n",
    "                # Aplicar sigmoide (salida continua entre 0 y 1)\n",
    "                output = self.sigmoid(z)\n",
    "                \n",
    "                # Clasificacion binaria con umbral 0.5\n",
    "                y_pred = 1 if output >= 0.5 else 0\n",
    "                \n",
    "                # Calcular error\n",
    "                error = yi - output  # Usamos la salida continua para la actualizacion\n",
    "                \n",
    "                # Actualizar pesos usando la regla del perceptron\n",
    "                # con la salida continua de la sigmoide\n",
    "                self.weights_ += self.learning_rate * error * xi\n",
    "                self.bias_ += self.learning_rate * error\n",
    "                \n",
    "                # Contar errores de clasificacion\n",
    "                errores_epoca += int(yi != y_pred)\n",
    "                loss_epoca += (yi - output) ** 2\n",
    "            \n",
    "            self.errors_.append(errores_epoca)\n",
    "            self.losses_.append(loss_epoca / n_muestras)  # MSE\n",
    "            \n",
    "            if errores_epoca == 0:\n",
    "                print(f\"Convergencia alcanzada en la epoca {epoca + 1}\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predice la clase usando umbral 0.5 sobre la sigmoide.\"\"\"\n",
    "        z = np.dot(X, self.weights_) + self.bias_\n",
    "        output = self.sigmoid(z)\n",
    "        return np.where(output >= 0.5, 1, 0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Retorna la probabilidad (salida de la sigmoide).\"\"\"\n",
    "        z = np.dot(X, self.weights_) + self.bias_\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "\n",
    "# Entrenar el Perceptron con sigmoide\n",
    "perceptron_sig = PerceptronSigmoide(learning_rate=0.01, n_epochs=100)\n",
    "perceptron_sig.fit(X, y)\n",
    "\n",
    "# Calcular accuracy\n",
    "acc_sig = accuracy_score(y, perceptron_sig.predict(X))\n",
    "print(f\"\\nAccuracy del Perceptron con Sigmoide: {acc_sig * 100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# COMPARACION: ESCALON vs SIGMOIDE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Perceptron con funcion escalon\n",
    "acc_step = accuracy_score(y, perceptron.predict(X))\n",
    "plot_decision_boundary(\n",
    "    X, y, perceptron,\n",
    "    title=f'Perceptron (Escalon)\\nAccuracy = {acc_step*100:.1f}%',\n",
    "    ax=axes[0]\n",
    ")\n",
    "\n",
    "# Perceptron con sigmoide\n",
    "acc_sig = accuracy_score(y, perceptron_sig.predict(X))\n",
    "plot_decision_boundary(\n",
    "    X, y, perceptron_sig,\n",
    "    title=f'Perceptron (Sigmoide)\\nAccuracy = {acc_sig*100:.1f}%',\n",
    "    ax=axes[1]\n",
    ")\n",
    "\n",
    "plt.suptitle('Comparacion: Funcion Escalon vs Sigmoide', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparacion de convergencia:\")\n",
    "print(f\"  Escalon  -> Epocas: {len(perceptron.errors_)}, Accuracy: {acc_step*100:.1f}%\")\n",
    "print(f\"  Sigmoide -> Epocas: {len(perceptron_sig.errors_)}, Accuracy: {acc_sig*100:.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# COMPARACION DE LOSS (ERROR CUADRATICO MEDIO)\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(range(1, len(perceptron_sig.losses_) + 1), perceptron_sig.losses_,\n",
    "        marker='o', color='#3498db', linewidth=2, markersize=4, label='MSE - Sigmoide')\n",
    "ax.plot(range(1, len(perceptron.errors_) + 1),\n",
    "        [e / len(y) for e in perceptron.errors_],\n",
    "        marker='s', color='#e74c3c', linewidth=2, markersize=4, label='Proporcion de errores - Escalon')\n",
    "\n",
    "ax.set_xlabel('Epoca', fontsize=14)\n",
    "ax.set_ylabel('Error', fontsize=14)\n",
    "ax.set_title('Convergencia: Escalon vs Sigmoide', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta del Experimento 3\n",
    "\n",
    "**\\u00bfComo cambia la frontera de decision al usar sigmoide vs escalon? \\u00bfQue ventaja tiene la sigmoide?**\n",
    "\n",
    "Pistas para tu reflexion:\n",
    "- \\u00bfLa frontera de decision es muy diferente visualmente?\n",
    "- Piensa en la diferenciabilidad: \\u00bfpor que es importante que una funcion sea diferenciable para el aprendizaje?\n",
    "- \\u00bfQue informacion adicional nos da la sigmoide que el escalon no nos da? (Pista: piensa en probabilidades)\n",
    "- \\u00bfCual crees que es mas util para entrenar redes neuronales profundas y por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 4: La Limitacion del Perceptron - Problema XOR\n",
    "\n",
    "Este es uno de los experimentos mas importantes de toda la guia. En 1969, Marvin Minsky y Seymour Papert publicaron el libro **\"Perceptrons\"**, donde demostraron matematicamente que un Perceptron simple **no puede resolver el problema XOR** (OR exclusivo).\n",
    "\n",
    "Esta limitacion casi \"mato\" a las redes neuronales como campo de investigacion durante mas de una decada (el famoso \"invierno de la IA\").\n",
    "\n",
    "#### \\u00bfQue es el problema XOR?\n",
    "\n",
    "La operacion XOR (OR exclusivo) es una operacion logica que retorna verdadero (1) solo cuando **exactamente una** de las entradas es verdadera:\n",
    "\n",
    "| $x_1$ | $x_2$ | XOR |\n",
    "|-------|-------|-----|\n",
    "| 0     | 0     | 0   |\n",
    "| 0     | 1     | 1   |\n",
    "| 1     | 0     | 1   |\n",
    "| 1     | 1     | 0   |\n",
    "\n",
    "El problema es que estos datos **no son linealmente separables**: no existe ninguna linea recta que pueda separar los 0s de los 1s."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 4: EL PROBLEMA XOR\n",
    "# ============================================================\n",
    "\n",
    "# Crear los datos XOR manualmente\n",
    "X_xor = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y_xor = np.array([0, 1, 1, 0])  # Resultado de la operacion XOR\n",
    "\n",
    "# Visualizar los datos XOR\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Puntos de clase 0 (azul)\n",
    "ax.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],\n",
    "           c='blue', s=300, edgecolors='k', linewidth=2, label='Clase 0', zorder=5)\n",
    "\n",
    "# Puntos de clase 1 (rojo)\n",
    "ax.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],\n",
    "           c='red', s=300, edgecolors='k', linewidth=2, label='Clase 1', zorder=5)\n",
    "\n",
    "# Anotar cada punto con su etiqueta\n",
    "for i in range(len(X_xor)):\n",
    "    ax.annotate(f'({X_xor[i,0]}, {X_xor[i,1]}) -> {y_xor[i]}',\n",
    "                xy=(X_xor[i, 0], X_xor[i, 1]),\n",
    "                xytext=(X_xor[i, 0] + 0.1, X_xor[i, 1] + 0.1),\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('$x_1$', fontsize=16)\n",
    "ax.set_ylabel('$x_2$', fontsize=16)\n",
    "ax.set_title('Problema XOR\\n(No es linealmente separable)', fontsize=18)\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observa que NO existe ninguna linea recta que separe\")\n",
    "print(\"los puntos azules (clase 0) de los rojos (clase 1).\")\n",
    "print(\"\\nLos puntos de la misma clase estan en esquinas OPUESTAS.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# INTENTAR ENTRENAR EL PERCEPTRON CON DATOS XOR\n",
    "# ============================================================\n",
    "\n",
    "# Entrenar el perceptron con los datos XOR\n",
    "# Usamos muchas epocas para dar tiempo suficiente\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, n_epochs=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Verificar las predicciones\n",
    "print(\"\\nResultados del Perceptron en el problema XOR:\")\n",
    "print(\"-\" * 45)\n",
    "predicciones_xor = perceptron_xor.predict(X_xor)\n",
    "\n",
    "for i in range(len(X_xor)):\n",
    "    correcto = \"OK\" if predicciones_xor[i] == y_xor[i] else \"ERROR\"\n",
    "    print(f\"  Entrada: {X_xor[i]} | Real: {y_xor[i]} | Predicho: {predicciones_xor[i]} | {correcto}\")\n",
    "\n",
    "acc_xor = accuracy_score(y_xor, predicciones_xor)\n",
    "print(f\"\\nAccuracy: {acc_xor * 100:.1f}%\")\n",
    "print(f\"Epocas usadas: {len(perceptron_xor.errors_)} (maximo: 100)\")\n",
    "\n",
    "if acc_xor < 1.0:\n",
    "    print(\"\\nEl Perceptron NO logro resolver el problema XOR.\")\n",
    "    print(\"Esto confirma la limitacion teorica descrita por Minsky y Papert.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION: POR QUE FALLA EL PERCEPTRON CON XOR\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# --- Panel 1: Problema AND (si se puede) ---\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])  # AND\n",
    "\n",
    "p_and = Perceptron(learning_rate=0.1, n_epochs=100)\n",
    "p_and.fit(X_and, y_and)\n",
    "\n",
    "# Crear malla para la frontera de decision\n",
    "xx, yy = np.meshgrid(np.arange(-0.5, 1.6, 0.01), np.arange(-0.5, 1.6, 0.01))\n",
    "Z_and = p_and.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z_and, alpha=0.3, cmap='bwr')\n",
    "axes[0].contour(xx, yy, Z_and, colors='k', linewidths=2)\n",
    "axes[0].scatter(X_and[y_and==0, 0], X_and[y_and==0, 1], c='blue', s=300, edgecolors='k', linewidth=2)\n",
    "axes[0].scatter(X_and[y_and==1, 0], X_and[y_and==1, 1], c='red', s=300, edgecolors='k', linewidth=2)\n",
    "axes[0].set_title('AND (linealmente separable)', fontsize=14)\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "\n",
    "# --- Panel 2: Problema OR (si se puede) ---\n",
    "y_or = np.array([0, 1, 1, 1])  # OR\n",
    "\n",
    "p_or = Perceptron(learning_rate=0.1, n_epochs=100)\n",
    "p_or.fit(X_and, y_or)\n",
    "\n",
    "Z_or = p_or.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z_or, alpha=0.3, cmap='bwr')\n",
    "axes[1].contour(xx, yy, Z_or, colors='k', linewidths=2)\n",
    "axes[1].scatter(X_and[y_or==0, 0], X_and[y_or==0, 1], c='blue', s=300, edgecolors='k', linewidth=2)\n",
    "axes[1].scatter(X_and[y_or==1, 0], X_and[y_or==1, 1], c='red', s=300, edgecolors='k', linewidth=2)\n",
    "axes[1].set_title('OR (linealmente separable)', fontsize=14)\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "\n",
    "# --- Panel 3: Problema XOR (NO se puede) ---\n",
    "Z_xor = perceptron_xor.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "axes[2].contourf(xx, yy, Z_xor, alpha=0.3, cmap='bwr')\n",
    "axes[2].contour(xx, yy, Z_xor, colors='k', linewidths=2)\n",
    "axes[2].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=300, edgecolors='k', linewidth=2)\n",
    "axes[2].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=300, edgecolors='k', linewidth=2)\n",
    "axes[2].set_title('XOR (NO linealmente separable)', fontsize=14, color='red')\n",
    "axes[2].set_xlabel('$x_1$')\n",
    "axes[2].set_ylabel('$x_2$')\n",
    "\n",
    "plt.suptitle('\\u00bfPor que falla el Perceptron con XOR?', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserva que:\")\n",
    "print(\"  - AND y OR son linealmente separables (una linea puede separarlos).\")\n",
    "print(\"  - XOR NO es linealmente separable (necesitarias una curva o dos lineas).\")\n",
    "print(\"  - El Perceptron solo puede aprender fronteras LINEALES.\")\n",
    "print(\"  - Por eso, el Perceptron resuelve AND y OR, pero FALLA con XOR.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GRAFICA DE ERRORES DEL PERCEPTRON EN EL PROBLEMA XOR\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(range(1, len(perceptron_xor.errors_) + 1), perceptron_xor.errors_,\n",
    "        marker='o', color='#e74c3c', linewidth=2, markersize=4)\n",
    "\n",
    "ax.set_xlabel('Epoca', fontsize=14)\n",
    "ax.set_ylabel('Numero de errores', fontsize=14)\n",
    "ax.set_title('Errores del Perceptron en el Problema XOR\\n(Nunca converge a cero)', fontsize=16)\n",
    "ax.axhline(y=0, color='green', linewidth=2, linestyle='--', label='Convergencia (0 errores)')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observa como los errores OSCILAN y nunca llegan a cero.\")\n",
    "print(\"Esto demuestra que el Perceptron no puede resolver XOR.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta del Experimento 4\n",
    "\n",
    "**\\u00bfPor que el Perceptron NO puede resolver el problema XOR? \\u00bfQue solucion propondrias?** (Pista: piensa en lo que veremos en la proxima guia)\n",
    "\n",
    "Pistas para tu reflexion:\n",
    "- \\u00bfQue tipo de frontera de decision crea el Perceptron? (lineal o no lineal)\n",
    "- \\u00bfQue necesitarias para poder separar los datos XOR?\n",
    "- Si pudieras apilar dos perceptrones, \\u00bfcrees que podrias resolver XOR?\n",
    "- Piensa en como podrias combinar operaciones AND, OR y NOT para construir XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 5: Perceptron Multicapa - Una Mirada Anticipada\n",
    "\n",
    "Acabamos de ver que el Perceptron simple no puede resolver problemas no linealmente separables como XOR. La solucion historica a este problema fue **apilar multiples perceptrones en capas**, creando lo que se conoce como **Perceptron Multicapa** (MLP - Multi-Layer Perceptron).\n",
    "\n",
    "Un MLP tiene:\n",
    "- **Capa de entrada:** Recibe los datos.\n",
    "- **Capa(s) oculta(s):** Capas intermedias que transforman los datos.\n",
    "- **Capa de salida:** Produce la prediccion final.\n",
    "\n",
    "La idea clave es que las capas ocultas pueden aprender **representaciones intermedias** de los datos que **si** son linealmente separables, incluso si los datos originales no lo eran.\n",
    "\n",
    "En la siguiente guia aprenderemos a implementar un MLP desde cero. Por ahora, veamos como `sklearn` resuelve el problema XOR con un MLP."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 5: MLP RESOLVIENDO XOR CON SKLEARN\n",
    "# ============================================================\n",
    "\n",
    "# Crear un MLPClassifier con una capa oculta de 4 neuronas\n",
    "mlp_xor = MLPClassifier(\n",
    "    hidden_layer_sizes=(4,),    # 1 capa oculta con 4 neuronas\n",
    "    activation='relu',          # Funcion de activacion ReLU\n",
    "    solver='adam',              # Optimizador Adam\n",
    "    max_iter=5000,              # Maximo de iteraciones\n",
    "    random_state=42,            # Semilla para reproducibilidad\n",
    "    learning_rate_init=0.01     # Tasa de aprendizaje inicial\n",
    ")\n",
    "\n",
    "# Entrenar el MLP con los datos XOR\n",
    "mlp_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Verificar predicciones\n",
    "predicciones_mlp = mlp_xor.predict(X_xor)\n",
    "\n",
    "print(\"Resultados del MLPClassifier en el problema XOR:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X_xor)):\n",
    "    correcto = \"OK\" if predicciones_mlp[i] == y_xor[i] else \"ERROR\"\n",
    "    print(f\"  Entrada: {X_xor[i]} | Real: {y_xor[i]} | Predicho: {predicciones_mlp[i]} | {correcto}\")\n",
    "\n",
    "acc_mlp = accuracy_score(y_xor, predicciones_mlp)\n",
    "print(f\"\\nAccuracy: {acc_mlp * 100:.1f}%\")\n",
    "\n",
    "# Informacion de la arquitectura\n",
    "print(f\"\\nArquitectura de la red:\")\n",
    "print(f\"  Capa de entrada: {X_xor.shape[1]} neuronas (una por feature)\")\n",
    "print(f\"  Capas ocultas: {mlp_xor.hidden_layer_sizes}\")\n",
    "print(f\"  Capa de salida: 1 neurona (clasificacion binaria)\")\n",
    "print(f\"  Funcion de activacion: {mlp_xor.activation}\")\n",
    "print(f\"  Iteraciones realizadas: {mlp_xor.n_iter_}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# COMPARACION VISUAL: PERCEPTRON vs MLP EN XOR\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Malla para la visualizacion\n",
    "xx, yy = np.meshgrid(np.arange(-0.5, 1.6, 0.01), np.arange(-0.5, 1.6, 0.01))\n",
    "\n",
    "# --- Panel 1: Perceptron simple ---\n",
    "Z_perc = perceptron_xor.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[0].contourf(xx, yy, Z_perc, alpha=0.3, cmap='bwr')\n",
    "axes[0].contour(xx, yy, Z_perc, colors='k', linewidths=2)\n",
    "axes[0].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=300, edgecolors='k', linewidth=2)\n",
    "axes[0].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=300, edgecolors='k', linewidth=2)\n",
    "axes[0].set_title(f'Perceptron Simple\\nAccuracy: {accuracy_score(y_xor, perceptron_xor.predict(X_xor))*100:.0f}%',\n",
    "                  fontsize=16, color='red')\n",
    "axes[0].set_xlabel('$x_1$', fontsize=14)\n",
    "axes[0].set_ylabel('$x_2$', fontsize=14)\n",
    "\n",
    "# --- Panel 2: MLP ---\n",
    "Z_mlp = mlp_xor.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[1].contourf(xx, yy, Z_mlp, alpha=0.3, cmap='bwr')\n",
    "axes[1].contour(xx, yy, Z_mlp, colors='k', linewidths=2)\n",
    "axes[1].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=300, edgecolors='k', linewidth=2)\n",
    "axes[1].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=300, edgecolors='k', linewidth=2)\n",
    "axes[1].set_title(f'Perceptron Multicapa (MLP)\\nAccuracy: {acc_mlp*100:.0f}%',\n",
    "                  fontsize=16, color='green')\n",
    "axes[1].set_xlabel('$x_1$', fontsize=14)\n",
    "axes[1].set_ylabel('$x_2$', fontsize=14)\n",
    "\n",
    "plt.suptitle('Perceptron Simple vs MLP en el Problema XOR', fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserva la diferencia:\")\n",
    "print(\"  - El Perceptron simple crea una frontera LINEAL (una sola linea recta).\")\n",
    "print(\"  - El MLP crea una frontera NO LINEAL (puede ser curva o usar varias lineas).\")\n",
    "print(\"  - Gracias a la capa oculta, el MLP puede separar correctamente los datos XOR.\")\n",
    "print(\"\\nEn la proxima guia, aprenderemos a construir un MLP desde cero.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta del Experimento 5\n",
    "\n",
    "**\\u00bfCuantas capas y neuronas uso el MLPClassifier para resolver XOR? \\u00bfPor que una sola capa no era suficiente?**\n",
    "\n",
    "Pistas para tu reflexion:\n",
    "- Revisa la arquitectura impresa arriba.\n",
    "- \\u00bfQue rol cumple la capa oculta? \\u00bfQue representaciones intermedias podria estar aprendiendo?\n",
    "- Piensa en como XOR se puede descomponer en operaciones mas simples: $XOR(x_1, x_2) = AND(OR(x_1, x_2), NAND(x_1, x_2))$.\n",
    "- \\u00bfCuantas neuronas necesitarias al minimo para representar cada una de esas operaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sintesis\n",
    "\n",
    "Has completado los cinco experimentos de esta guia. Ahora es momento de consolidar lo que has aprendido con algunas preguntas de reflexion final. Estas preguntas son **evaluativas**, asi que tomate el tiempo para responderlas con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 1\n",
    "\n",
    "**Resume en tus propias palabras que es un Perceptron y cuales son sus limitaciones.**\n",
    "\n",
    "Tu respuesta deberia incluir:\n",
    "- \\u00bfQue componentes tiene un Perceptron? (entradas, pesos, bias, funcion de activacion, salida)\n",
    "- \\u00bfComo aprende? (regla de actualizacion de pesos)\n",
    "- \\u00bfQue tipo de problemas puede resolver?\n",
    "- \\u00bfCual es su principal limitacion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 2\n",
    "\n",
    "**\\u00bfEn que tipo de problemas reales podria funcionar un Perceptron simple?**\n",
    "\n",
    "Piensa en situaciones donde los datos sean linealmente separables. Algunos ejemplos para inspirarte:\n",
    "- Clasificacion de correos (spam vs no spam) si las caracteristicas son simples.\n",
    "- Deteccion basica (si/no) basada en umbrales.\n",
    "- Decisiones binarias simples.\n",
    "\n",
    "Da al menos 3 ejemplos concretos y explica por que crees que un Perceptron podria funcionar en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 3\n",
    "\n",
    "**\\u00bfQue esperas aprender en la siguiente guia sobre redes multicapa?**\n",
    "\n",
    "Basandote en lo que viste en el Experimento 5 (MLP resolviendo XOR), piensa:\n",
    "- \\u00bfQue preguntas te quedan por resolver?\n",
    "- \\u00bfComo crees que se entrenan las capas ocultas? (Pista: backpropagation)\n",
    "- \\u00bfQue nuevas posibilidades abre tener multiples capas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Reto Extra\n",
    "\n",
    "### Clasificacion de Flores con el Perceptron\n",
    "\n",
    "Ahora que entiendes como funciona el Perceptron, vamos a aplicarlo a un dataset real: el famoso **dataset Iris** de R.A. Fisher.\n",
    "\n",
    "El dataset Iris contiene mediciones de 150 flores de 3 especies:\n",
    "- *Iris setosa*\n",
    "- *Iris versicolor*\n",
    "- *Iris virginica*\n",
    "\n",
    "Para este reto, vamos a simplificar el problema:\n",
    "- Usar **solo 2 clases**: setosa vs versicolor (para que sea clasificacion binaria).\n",
    "- Usar **solo 2 features**: largo del petalo y ancho del petalo (para poder visualizar en 2D).\n",
    "\n",
    "#### Instrucciones paso a paso:\n",
    "\n",
    "1. Cargar el dataset Iris con `load_iris()`.\n",
    "2. Seleccionar solo las clases 0 (setosa) y 1 (versicolor).\n",
    "3. Seleccionar solo las features 2 y 3 (largo y ancho del petalo).\n",
    "4. Dividir en entrenamiento y prueba (80/20).\n",
    "5. Entrenar tu Perceptron.\n",
    "6. Evaluar el accuracy en el conjunto de prueba.\n",
    "7. Visualizar la frontera de decision.\n",
    "\n",
    "El codigo de abajo tiene partes completadas y partes que **debes completar tu**. Las partes incompletas estan marcadas con `### TU CODIGO AQUI ###`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# RETO EXTRA: PERCEPTRON CON DATASET IRIS\n",
    "# ============================================================\n",
    "\n",
    "# Paso 1: Cargar el dataset Iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Paso 2: Seleccionar solo las clases 0 (setosa) y 1 (versicolor)\n",
    "# Recuerda que iris.target contiene las etiquetas (0, 1, 2)\n",
    "mascara = iris.target < 2  # True para clases 0 y 1, False para clase 2\n",
    "\n",
    "# Paso 3: Seleccionar solo las features 2 y 3 (largo y ancho del petalo)\n",
    "# iris.data tiene 4 columnas: [sepal_length, sepal_width, petal_length, petal_width]\n",
    "X_iris = iris.data[mascara][:, 2:4]  # Solo features de petalo\n",
    "y_iris = iris.target[mascara]         # Solo clases 0 y 1\n",
    "\n",
    "print(f\"Forma del dataset: X = {X_iris.shape}, y = {y_iris.shape}\")\n",
    "print(f\"Features usadas: {iris.feature_names[2]}, {iris.feature_names[3]}\")\n",
    "print(f\"Clases: {iris.target_names[0]} (0), {iris.target_names[1]} (1)\")\n",
    "\n",
    "# Paso 4: Dividir en entrenamiento y prueba\n",
    "### TU CODIGO AQUI ###\n",
    "# Usa train_test_split con test_size=0.2 y random_state=42\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "\n",
    "# Paso 5: Entrenar el Perceptron\n",
    "### TU CODIGO AQUI ###\n",
    "# Crea una instancia de la clase Perceptron con los hiperparametros que consideres apropiados\n",
    "# Entrena con X_train y y_train\n",
    "\n",
    "\n",
    "# Paso 6: Evaluar el accuracy\n",
    "### TU CODIGO AQUI ###\n",
    "# Calcula y muestra el accuracy tanto en entrenamiento como en prueba\n",
    "\n",
    "\n",
    "# Paso 7: Visualizar la frontera de decision\n",
    "### TU CODIGO AQUI ###\n",
    "# Usa la funcion plot_decision_boundary() que definimos arriba\n",
    "# Grafica usando X_train y y_train\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Referencias\n",
    "\n",
    "### Documentacion oficial:\n",
    "- [NumPy - Documentacion oficial](https://numpy.org/doc/stable/)\n",
    "- [Matplotlib - Documentacion oficial](https://matplotlib.org/stable/)\n",
    "- [Scikit-learn - MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "- [Scikit-learn - make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)\n",
    "\n",
    "### Recursos de aprendizaje:\n",
    "- [3Blue1Brown - Neural Networks (YouTube)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Excelente serie visual sobre redes neuronales.\n",
    "- [Deep Learning Book - Ian Goodfellow](https://www.deeplearningbook.org/) - Libro de referencia gratuito en linea.\n",
    "- [Stanford CS231n - Convolutional Neural Networks](http://cs231n.stanford.edu/) - Curso clasico de Stanford.\n",
    "\n",
    "### Contexto historico:\n",
    "- Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. *Psychological Review*, 65(6), 386-408.\n",
    "- Minsky, M., & Papert, S. (1969). *Perceptrons: An Introduction to Computational Geometry*. MIT Press.\n",
    "\n",
    "---\n",
    "\n",
    "**Fin de la Guia 01** \n",
    "\n",
    "En la **Guia 02** aprenderemos a construir un **Perceptron Multicapa (MLP)** desde cero, incluyendo el algoritmo de **backpropagation** para entrenar las capas ocultas. Esto nos permitira resolver problemas como XOR y muchos mas.\n",
    "\n",
    "---\n",
    "\n",
    "*Electiva II - Deep Learning | Tecnologico de Antioquia | 2026-1*"
   ]
  }
 ]
}